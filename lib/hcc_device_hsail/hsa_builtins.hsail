
////////////////////////////////////////////////////////////
/// HSAIL builtin functions
////////////////////////////////////////////////////////////

/// get wavefront size
prog function &__wavesize(arg_u32 %dest)()
{
  st_arg_u32 WAVESIZE, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 5.6 : Individual Bit Instructions
////////////////////////////////////////////////////////////

/// popcount_u32_b32
prog function &__popcount_u32_b32(arg_u32 %dest)(arg_u32 %src)
{
  ld_arg_u32 $s0, [%src];
  popcount_u32_b32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// popcount_u32_b64
prog function &__popcount_u32_b64(arg_u32 %dest)(arg_u64 %src)
{
  ld_arg_u64 $d0, [%src];
  popcount_u32_b64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 5.7 : Bit String Instructions
////////////////////////////////////////////////////////////

////////////////////
/// bitextract
////////////////////

/// bitextract_u32
prog function &__bitextract_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bitextract_u32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// bitextract_u64
prog function &__bitextract_u64(arg_u64 %dest)(arg_u64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  bitextract_u64 $d1, $d0, $s0, $s1;
  st_arg_u64 $d1, [%dest];
  ret;
};

/// bitextract_s32
prog function &__bitextract_s32(arg_s32 %dest)(arg_s32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bitextract_s32 $s3, $s0, $s1, $s2;
  st_arg_s32 $s3, [%dest];
  ret;
};

/// bitextract_s64
prog function &__bitextract_s64(arg_s64 %dest)(arg_s64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  bitextract_s64 $d1, $d0, $s0, $s1;
  st_arg_s64 $d1, [%dest];
  ret;
};

////////////////////
/// bitinsert
////////////////////

/// bitinsert_u32
prog function &__bitinsert_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2, arg_u32 %src3)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  ld_arg_u32 $s3, [%src3];
  bitinsert_u32 $s4, $s0, $s1, $s2, $s3;
  st_arg_u32 $s4, [%dest];
  ret;
};

/// bitinsert_u64
prog function &__bitinsert_u64(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1, arg_u32 %src2, arg_u32 %src3)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  ld_arg_u32 $s0, [%src2];
  ld_arg_u32 $s1, [%src3];
  bitinsert_u64 $d2, $d0, $d1, $s0, $s1;
  st_arg_u64 $d2, [%dest];
  ret;
};

/// bitinsert_s32
prog function &__bitinsert_s32(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1, arg_u32 %src2, arg_u32 %src3)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  ld_arg_u32 $s3, [%src3];
  bitinsert_u32 $s4, $s0, $s1, $s2, $s3;
  st_arg_s32 $s4, [%dest];
  ret;
};

/// bitinsert_s64
prog function &__bitinsert_s64(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1, arg_u32 %src2, arg_u32 %src3)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  ld_arg_u32 $s0, [%src2];
  ld_arg_u32 $s1, [%src3];
  bitinsert_s64 $d2, $d0, $d1, $s0, $s1;
  st_arg_s64 $d2, [%dest];
  ret;
};

////////////////////
/// bitmask
////////////////////

/// bitmask_b32
prog function &__bitmask_b32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  bitmask_b32 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

/// bitmask_b64
prog function &__bitmask_b64(arg_u64 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  bitmask_b64 $d0, $s0, $s1;
  st_arg_u64 $d0, [%dest];
  ret;
};

////////////////////
/// bitrev
////////////////////

/// bitrev_b32
prog function &__bitrev_b32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_u32 $s0, [%src0];
  bitrev_b32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// bitrev_b64
prog function &__bitrev_b64(arg_u64 %dest)(arg_u64 %src0)
{
  ld_arg_u64 $d0, [%src0];
  bitrev_b64 $d1, $d0;
  st_arg_u64 $d1, [%dest];
  ret;
};

////////////////////
/// bitselect
////////////////////

/// bitselect_b32
prog function &__bitselect_b32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bitselect_b32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// bitselect_b64
prog function &__bitselect_b64(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1, arg_u64 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  ld_arg_u64 $d2, [%src2];
  bitselect_b64 $d3, $d0, $d1, $d2;
  st_arg_u64 $d3, [%dest];
  ret;
};

////////////////////
/// firstbit
////////////////////

/// firstbit_u32_u32
prog function &__firstbit_u32_u32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_u32 $s0, [%src0];
  firstbit_u32_u32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// firstbit_u32_u64
prog function &__firstbit_u32_u64(arg_u32 %dest)(arg_u64 %src0)
{
  ld_arg_u64 $d0, [%src0];
  firstbit_u32_u64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

/// firstbit_u32_s32
prog function &__firstbit_u32_s32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_s32 $s0, [%src0];
  firstbit_u32_s32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// firstbit_u32_s64
prog function &__firstbit_u32_s64(arg_u32 %dest)(arg_u64 %src0)
{
  ld_arg_s64 $d0, [%src0];
  firstbit_u32_s64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

////////////////////
/// lastbit
////////////////////

/// lastbit_u32_u32
prog function &__lastbit_u32_u32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_u32 $s0, [%src0];
  lastbit_u32_u32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// lastbit_u32_u64
prog function &__lastbit_u32_u64(arg_u32 %dest)(arg_u64 %src0)
{
  ld_arg_u64 $d0, [%src0];
  lastbit_u32_u64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

/// lastbit_u32_s32
prog function &__lastbit_u32_s32(arg_u32 %dest)(arg_u32 %src0)
{
  ld_arg_s32 $s0, [%src0];
  lastbit_u32_s32 $s1, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

/// lastbit_u32_s64
prog function &__lastbit_u32_s64(arg_u32 %dest)(arg_u64 %src0)
{
  ld_arg_s64 $d0, [%src0];
  lastbit_u32_s64 $s0, $d0;
  st_arg_u32 $s0, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 5.9 : Packed Data Instructions
////////////////////////////////////////////////////////////

////////////////////
/// shuffle
////////////////////

/// NOTE: shuffle can not be implemented as of now because src2 operand must be a constant value
/// and it's not possible to do so as a library function.
/// shuffle_u8x4
/// shuffle_u8x8
/// shuffle_u16x2
/// shuffle_u16x4
/// shuffle_u32x2
/// shuffle_s8x4
/// shuffle_s8x8
/// shuffle_s16x2
/// shuffle_s16x4
/// shuffle_s32x2
/// shuffle_f8x4
/// shuffle_f8x8
/// shuffle_f16x2
/// shuffle_f16x4
/// shuffle_f32x2

////////////////////
/// unpacklo
////////////////////

// unpacklo_u8x4
prog function &__unpacklo_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpacklo_u8x4 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpacklo_u8x8
prog function &__unpacklo_u8x8(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpacklo_u8x8 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpacklo_u16x2
prog function &__unpacklo_u16x2(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpacklo_u16x2 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpacklo_u16x4
prog function &__unpacklo_u16x4(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpacklo_u16x4 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpacklo_u32x2
prog function &__unpacklo_u32x2(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpacklo_u32x2 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpacklo_s8x4
prog function &__unpacklo_s8x4(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  unpacklo_s8x4 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpacklo_s8x8
prog function &__unpacklo_s8x8(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpacklo_s8x8 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

// unpacklo_s16x2
prog function &__unpacklo_s16x2(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  unpacklo_s16x2 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpacklo_s16x4
prog function &__unpacklo_s16x4(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpacklo_s16x4 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

// unpacklo_s32x2
prog function &__unpacklo_s32x2(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpacklo_s32x2 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

////////////////////
/// unpackhi
////////////////////

// unpackhi_u8x4
prog function &__unpackhi_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpackhi_u8x4 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpackhi_u8x8
prog function &__unpackhi_u8x8(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpackhi_u8x8 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpackhi_u16x2
prog function &__unpackhi_u16x2(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpackhi_u16x2 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpackhi_u16x4
prog function &__unpackhi_u16x4(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpackhi_u16x4 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpackhi_u32x2
prog function &__unpackhi_u32x2(arg_u64 %dest)(arg_u64 %src0, arg_u64 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u64 $d1, [%src1];
  unpackhi_u32x2 $d2, $d0, $d1;
  st_arg_u64 $d2, [%dest];
  ret;
};

// unpackhi_s8x4
prog function &__unpackhi_s8x4(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  unpackhi_s8x4 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpackhi_s8x8
prog function &__unpackhi_s8x8(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpackhi_s8x8 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

// unpackhi_s16x2
prog function &__unpackhi_s16x2(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  unpackhi_s16x2 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpackhi_s16x4
prog function &__unpackhi_s16x4(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpackhi_s16x4 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

// unpackhi_s32x2
prog function &__unpackhi_s32x2(arg_s64 %dest)(arg_s64 %src0, arg_s64 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s64 $d1, [%src1];
  unpackhi_s32x2 $d2, $d0, $d1;
  st_arg_s64 $d2, [%dest];
  ret;
};

////////////////////
/// pack
////////////////////

// pack_u8x4_u32
prog function &__pack_u8x4_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  pack_u8x4_u32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

// pack_u8x8_u32
prog function &__pack_u8x8_u32(arg_u64 %dest)(arg_u64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_u8x8_u32 $d1, $d0, $s0, $s1;
  st_arg_u64 $d1, [%dest];
  ret;
};

// pack_u8x16_u32 is not implemented as of now

// pack_u16x2_u32
prog function &__pack_u16x2_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  pack_u16x2_u32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

// pack_u16x4_u32
prog function &__pack_u16x4_u32(arg_u64 %dest)(arg_u64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_u16x4_u32 $d1, $d0, $s0, $s1;
  st_arg_u64 $d1, [%dest];
  ret;
};

// pack_u16x8_u32 is not implemented as of now

// pack_u32x2_u32
prog function &__pack_u32x2_u32(arg_u64 %dest)(arg_u64 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_u32x2_u32 $d1, $d0, $s0, $s1;
  st_arg_u64 $d1, [%dest];
  ret;
};

// pack_u32x4_u32 is not implemented as of now

// pack_u64x2_u32 is not implemented as of now

// pack_s8x4_s32
prog function &__pack_s8x4_s32(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  pack_s8x4_s32 $s3, $s0, $s1, $s2;
  st_arg_s32 $s3, [%dest];
  ret;
};

// pack_s8x8_s32
prog function &__pack_s8x8_s32(arg_s64 %dest)(arg_s64 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_s8x8_s32 $d1, $d0, $s0, $s1;
  st_arg_s64 $d1, [%dest];
  ret;
};

// pack_s8x16_s32 is not implemented as of now

// pack_s16x2_s32
prog function &__pack_s16x2_s32(arg_s32 %dest)(arg_s32 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_s32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  pack_s16x2_s32 $s3, $s0, $s1, $s2;
  st_arg_s32 $s3, [%dest];
  ret;
};

// pack_s16x4_s32
prog function &__pack_s16x4_s32(arg_u64 %dest)(arg_s64 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_s16x4_s32 $d1, $d0, $s0, $s1;
  st_arg_s64 $d1, [%dest];
  ret;
};

// pack_s16x8_s32 is not implemented as of now

// pack_s32x2_s32
prog function &__pack_s32x2_s32(arg_s64 %dest)(arg_s64 %src0, arg_s32 %src1, arg_u32 %src2)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_s32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_s32x2_s32 $d1, $d0, $s0, $s1;
  st_arg_s64 $d1, [%dest];
  ret;
};

// pack_s32x4_s32 is not implemented as of now

// pack_s64x2_s32 is not implemented as of now

// pack_f32x2_f32
prog function &__pack_f32x2_f32(arg_f64 %dest)(arg_f64 %src0, arg_f32 %src1, arg_u32 %src2)
{
  ld_arg_f64 $d0, [%src0];
  ld_arg_f32 $s0, [%src1];
  ld_arg_u32 $s1, [%src2];
  pack_f32x2_f32 $d1, $d0, $s0, $s1;
  st_arg_f64 $d1, [%dest];
  ret;
};

// pack_f32x4_f32 is not implemented as of now

// pack_f64x2_f32 is not implemented as of now

// pack_f16x2_f16 is not implemented as of now

// pack_f16x4_f16 is not implemented as of now

// pack_f16x8_f16 is not implemented as of now

////////////////////
/// unpack
////////////////////

// unpack_u32_u8x4
prog function &__unpack_u32_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpack_u32_u8x4 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpack_u32_u8x8
prog function &__unpack_u32_u8x8(arg_u32 %dest)(arg_u64 %src0, arg_u32 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_u32_u8x8 $s1, $d0, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

// unpack_u32_u8x16 is not implemented as of now

// unpack_u32_u16x2
prog function &__unpack_u32_u16x2(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpack_u32_u16x2 $s2, $s0, $s1;
  st_arg_u32 $s2, [%dest];
  ret;
};

// unpack_u32_u16x4
prog function &__unpack_u32_u16x4(arg_u32 %dest)(arg_u64 %src0, arg_u32 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_u32_u16x4 $s1, $d0, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

// unpack_u32_u16x8 is not implemented as of now

// unpack_u32_u32x2
prog function &__unpack_u32_u32x2(arg_u32 %dest)(arg_u64 %src0, arg_u32 %src1)
{
  ld_arg_u64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_u32_u32x2 $s1, $d0, $s0;
  st_arg_u32 $s1, [%dest];
  ret;
};

// unpack_u32_u32x4 is not implemented as of now

// unpack_u32_u64x2 is not implemented as of now

// unpack_s32_s8x4
prog function &__unpack_s32_s8x4(arg_s32 %dest)(arg_s32 %src0, arg_u32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpack_s32_s8x4 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpack_s32_s8x8
prog function &__unpack_s32_s8x8(arg_s32 %dest)(arg_s64 %src0, arg_u32 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_s32_s8x8 $s1, $d0, $s0;
  st_arg_s32 $s1, [%dest];
  ret;
};

// unpack_s32_s8x16 is not implemented as of now

// unpack_s32_s16x2
prog function &__unpack_s32_s16x2(arg_s32 %dest)(arg_s32 %src0, arg_u32 %src1)
{
  ld_arg_s32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  unpack_s32_s16x2 $s2, $s0, $s1;
  st_arg_s32 $s2, [%dest];
  ret;
};

// unpack_s32_s16x4
prog function &__unpack_s32_s16x4(arg_s32 %dest)(arg_s64 %src0, arg_u32 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_s32_s16x4 $s1, $d0, $s0;
  st_arg_s32 $s1, [%dest];
  ret;
};

// unpack_s32_s16x8 is not implemented as of now

// unpack_s32_s32x2
prog function &__unpack_s32_s32x2(arg_s32 %dest)(arg_s64 %src0, arg_u32 %src1)
{
  ld_arg_s64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_s32_s32x2 $s1, $d0, $s0;
  st_arg_s32 $s1, [%dest];
  ret;
};

// unpack_s32_s32x4 is not implemented as of now

// unpack_s32_s64x2 is not implemented as of now

// unpack_f32_f32x2
prog function &__unpack_f32_f32x2(arg_f32 %dest)(arg_f64 %src0, arg_u32 %src1)
{
  ld_arg_f64 $d0, [%src0];
  ld_arg_u32 $s0, [%src1];
  unpack_f32_f32x2 $s1, $d0, $s0;
  st_arg_f32 $s1, [%dest];
  ret;
};

// unpack_f32_f32x4 is not implemented as of now

// unpack_f32_f64x2 is not implemented as of now

// unpack_f16_f16x2 is not implemented as of now

// unpack_f16_f16x4 is not implemented as of now

// unpack_f16_f16x8 is not implemented as of now

////////////////////////////////////////////////////////////
/// PRM 5.15 : Multimedia Instructions
////////////////////////////////////////////////////////////

/// bitalign_b32
prog function &__bitalign_b32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bitalign_b32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// bytealign_b32
prog function &__bytealign_b32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  bytealign_b32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// lerp_u8x4
prog function &__lerp_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  lerp_u8x4 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// packcvt_u8x4_f32
prog function &__packcvt_u8x4_f32(arg_u32 %dest)(arg_f32 %src0, arg_f32 %src1, arg_f32 %src2, arg_f32 %src3)
{
  ld_arg_f32 $s0, [%src0];
  ld_arg_f32 $s1, [%src1];
  ld_arg_f32 $s2, [%src2];
  ld_arg_f32 $s3, [%src3];
  packcvt_u8x4_f32 $s4, $s0, $s1, $s2, $s3;
  st_arg_u32 $s4, [%dest];
  ret;
};

/// unpackcvt_f32_u8x4
/// NOTE:
/// - if src1 is larger or equal to 3, than unpackcvt with src1 as 3 would be used
prog function &__unpackcvt_f32_u8x4(arg_f32 %dest)(arg_f32 %src0, arg_u32 %src1)
{
  ld_arg_f32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  cmp_eq_b1_u32 $c0, $s1, 0;
  cbr_b1 $c0, @unpackcvt_0;
  cmp_eq_b1_u32 $c0, $s1, 1;
  cbr_b1 $c0, @unpackcvt_1;
  cmp_eq_b1_u32 $c0, $s1, 2;
  cbr_b1 $c0, @unpackcvt_2;

@unpackcvt_3:
  unpackcvt_f32_u8x4 $s2, $s0, 3;
  br @return;

@unpackcvt_0:
  unpackcvt_f32_u8x4 $s2, $s0, 0;
  br @return;

@unpackcvt_1:
  unpackcvt_f32_u8x4 $s2, $s0, 1;
  br @return;

@unpackcvt_2:
  unpackcvt_f32_u8x4 $s2, $s0, 2;

@return:
  st_arg_f32 $s2, [%dest];
  ret;
};


/// sad_u32_u32
prog function &__sad_u32_u32(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  sad_u32_u32 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// sad_u32_u16x2
prog function &__sad_u32_u16x2(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  sad_u32_u16x2 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// sad_u32_u8x4
prog function &__sad_u32_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  sad_u32_u8x4 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// sadhi_u16x2_u8x4
prog function &__sadhi_u16x2_u8x4(arg_u32 %dest)(arg_u32 %src0, arg_u32 %src1, arg_u32 %src2)
{
  ld_arg_u32 $s0, [%src0];
  ld_arg_u32 $s1, [%src1];
  ld_arg_u32 $s2, [%src2];
  sadhi_u16x2_u8x4 $s3, $s0, $s1, $s2;
  st_arg_u32 $s3, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 6.6 : Atomic Instructions
////////////////////////////////////////////////////////////

/// atomic_wrapinc (global)
prog function &__atomic_wrapinc_global(arg_u32 %old)(arg_u64 %addr, arg_u32 %val)
{
  ld_arg_u64 $d0, [%addr];
  ld_arg_u32 $s0, [%val];
  atomic_wrapinc_global_scar_system_u32 $s1, [$d0], $s0;
  st_arg_u32 $s1, [%old];
  ret;
};

/// atomic_wrapinc (local)
prog function &__atomic_wrapinc_local(arg_u32 %old)(arg_u32 %addr, arg_u32 %val)
{
  ld_arg_u32 $s0, [%addr];
  ld_arg_u32 $s1, [%val];
  atomic_wrapinc_group_scar_wg_u32 $s2, [$s0], $s1;
  st_arg_u32 $s2, [%old];
  ret;
};

/// atomic_wrapdec (global)
prog function &__atomic_wrapdec_global(arg_u32 %old)(arg_u64 %addr, arg_u32 %val)
{
  ld_arg_u64 $d0, [%addr];
  ld_arg_u32 $s0, [%val];
  atomic_wrapdec_global_scar_system_u32 $s1, [$d0], $s0;
  st_arg_u32 $s1, [%old];
  ret;
};

/// atomic_wrapdec (local)
prog function &__atomic_wrapdec_local(arg_u32 %old)(arg_u32 %addr, arg_u32 %val)
{
  ld_arg_u32 $s0, [%addr];
  ld_arg_u32 $s1, [%val];
  atomic_wrapdec_group_scar_wg_u32 $s2, [$s0], $s1;
  st_arg_u32 $s2, [%old];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 9.4 : Cross-Lane Instructions
////////////////////////////////////////////////////////////

/// activelanecount_width_u32_b1
/// NOTE:
/// - width is not in use as of now
/// - src is of type u32, and will be converted to b1
prog function &__activelanecount_u32_b1(arg_u32 %dest)(arg_u32 %src)
{
  ld_arg_u32 $s0, [%src];
  cmp_ne_b1_u32 $c0, $s0, 0;
  activelanecount_u32_b1 $s0, $c0;
  st_arg_u32 $s0, [%dest];
  ret;
};

/// activelaneid_width_u32
/// NOTE:
/// - width is not in use as of now
prog function &__activelaneid_u32(arg_u32 %dest)()
{
  activelaneid_u32 $s0;
  st_arg_u32 $s0, [%dest];
  ret;
};

/// activelanemask_v4_width_b64_b1
/// NOTE:
/// - width is not in use as of now
/// - only dest0 is returned as of now
/// - dest1, dest2, dest3 are not returned as of now
/// - input is of type u32, and will be converted to b1
prog function &__activelanemask_v4_b64_b1(arg_u64 %dest0)(arg_u32 %src)
{
  ld_arg_u32 $s0, [%src];
  cmp_ne_b1_u32 $c0, $s0, 0;
  activelanemask_v4_b64_b1 ($d0, $d1, $d2, $d3), $c0;
  st_arg_u64 $d0, [%dest0];
  ret;
};

/// activelanepermute_width_b1 is not implemented as of now
/// activelanepermute_width_b128 is not implemented as of now

/// activelanepermute_width_b32
/// NOTE:
/// - width is not in use as of now
/// - useIdentity is of type u32, and will be converted to b1
prog function &__activelanepermute_b32(arg_u32 %dest)(arg_u32 %src, arg_u32 %laneId, arg_u32 %identity, arg_u32 %useIdentity)
{
  ld_arg_u32 $s0, [%src];
  ld_arg_u32 $s1, [%laneId];
  ld_arg_u32 $s2, [%identity];
  ld_arg_u32 $s3, [%useIdentity];
  cmp_ne_b1_u32 $c0, $s3, 0;
  activelanepermute_b32 $s3, $s0, $s1, $s2, $c0;
  st_arg_u32 $s3, [%dest];
  ret;
};

/// activelanepermute_width_b64
/// NOTE:
/// - width is not in use as of now
/// - useIdentity is of type b32, and will be converted to b1
prog function &__activelanepermute_b64(arg_u64 %dest)(arg_u64 %src, arg_u32 %laneId, arg_u64 %identity, arg_u32 %useIdentity)
{
  ld_arg_u64 $d0, [%src];
  ld_arg_u32 $s0, [%laneId];
  ld_arg_u64 $d1, [%identity];
  ld_arg_u32 $s1, [%useIdentity];
  cmp_ne_b1_u32 $c0, $s1, 0;
  activelanepermute_b64 $d2, $d0, $s0, $d1, $c0;
  st_arg_u64 $d2, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// PRM 11.4 : Miscellaneous Instructions
////////////////////////////////////////////////////////////

/// get system timestamp
prog function &__clock_u64(arg_u64 %dest)() {
  clock_u64 $d0;
  st_arg_u64 $d0, [%dest];
  ret;
};

/// get hardware cycle count
/// NOTE:
/// - There is no corresponding HSAIL instruction so we always return 0 here
prog function &__cycle_u64(arg_u64 %dest)() {
  st_arg_u64 0, [%dest];
  ret;
};

////////////////////////////////////////////////////////////
/// Group segment
////////////////////////////////////////////////////////////

/// get_group_segment_size : return the size of group segment, including both
/// static and dynamic group segment
/// XXX this function has been disabled in HSAIL path so it always return 0
prog function &get_group_segment_size(arg_u32 %ret)()
{
  st_arg_u32 0, [%ret];
  ret;
};

/// get_static_group_segment_size : return the size of static group segment
/// XXX this function has been disabled in HSAIL path so it always return 0
prog function &get_static_group_segment_size(arg_u32 %ret)()
{
  st_arg_u32 0, [%ret];
  ret;
};

/// get_group_segment_base_pointer : get the pointer to the beginning of group
/// segment
prog function &get_group_segment_base_pointer(arg_u32 %ret)()
{
  groupbaseptr_u32 $s0;

  add_u32 $s0, $s0, $s1;

  st_arg_u32 $s0, [%ret];
  ret;
};

/// get_dynamic_group_segment : get the pointer to the beginning of dynamic
/// group segment
prog function &get_dynamic_group_segment_base_pointer(arg_u32 %ret)()
{
  // call get_group_segment_base_pointer
  {
    arg_u32 %res1;
    call &get_group_segment_base_pointer(%res1)();
    ld_arg_u32 $s0, [%res1];
  }
  // call get_static_group_segment_size
  {
    arg_u32 %res2;
    call &get_static_group_segment_size(%res2)();
    ld_arg_u32 $s1, [%res2];
  }
  // calculate the result
  add_u32 $s1, $s0, $s1;
  st_arg_u32 $s1, [%ret];
  ret;
};

